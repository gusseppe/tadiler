{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70b66f3c-4035-4409-ac84-56f146fe543a",
   "metadata": {},
   "source": [
    "# Experiments: Diabetes Retinopathy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc5ecc2-e4b9-44cd-a5a6-47f819a3d085",
   "metadata": {},
   "source": [
    "<img src = 'https://repository-images.githubusercontent.com/195603342/63983100-b4a6-11e9-846c-99b9465f7b3b'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3342833b-c995-4564-acc1-c1e179703b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from docarray.typing import TorchTensor, TorchEmbedding, ImageUrl\n",
    "from typing import Optional\n",
    "from docarray.documents import ImageDoc\n",
    "from docarray import BaseDoc, DocVec, DocList\n",
    "from docarray.typing import ID\n",
    "\n",
    "class MultiModal(BaseDoc):\n",
    "    embedding: TorchTensor\n",
    "    path: ImageUrl\n",
    "    label: int\n",
    "    label_description: str\n",
    "    zeroshot_label: int\n",
    "    zeroshot_description: str\n",
    "    task: int\n",
    "    task_description: str\n",
    "    id_code: str\n",
    "    metadata: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b034078-e271-4aae-9aff-05d517fa9f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import clip\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from rich import print\n",
    "from scipy.stats import trim_mean\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def stratified_sampling(docs: DocList[MultiModal], zeroshot_labels, n_neighbors=10):\n",
    "    np.random.seed(43)\n",
    "# Stratified\n",
    "    # Create a list to store nearest docs\n",
    "    sampled_docs = []\n",
    "\n",
    "    # For each unique class in labels\n",
    "    for i in np.unique(zeroshot_labels):\n",
    "        # Filter instances belonging to current class\n",
    "        class_docs = [doc for doc, label in zip(docs, zeroshot_labels) if label == i]\n",
    "\n",
    "        # Sample n_samples_per_class from current class_docs\n",
    "        if len(class_docs) > n_neighbors:\n",
    "            sampled_docs += np.random.choice(class_docs, n_neighbors, replace=False).tolist()\n",
    "        else:\n",
    "            # Maybe the under-represented classes are the wrong one\n",
    "            sampled_docs += class_docs\n",
    "\n",
    "    # print(f\"length of sampled_docs = {len(sampled_docs)}\")\n",
    "\n",
    "    return DocList[MultiModal](sampled_docs)\n",
    "\n",
    "def zeroshot_clustering(docs: DocList[MultiModal], text_embs):\n",
    "    # Convert embeddings to a stack of tensors\n",
    "    image_embs = torch.stack([doc.embedding for doc in docs])\n",
    "    image_embs /= image_embs.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    text_emb = text_embs.clone()\n",
    "    text_emb /= text_emb.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    print(f\"image_embs: {image_embs.shape}\")\n",
    "    print(f\"text_emb: {text_emb.shape}\")\n",
    "    \n",
    "    # Compute the similarity matrix between all image embeddings and text embeddings\n",
    "    similarity = (100.0 * image_embs @ text_emb.T).softmax(dim=-1)\n",
    "    \n",
    "    # Find the index of the label with maximum similarity for each image embedding\n",
    "    zeroshot_labels = torch.argmax(similarity, dim=1).numpy()\n",
    "\n",
    "    # Number of unique labels\n",
    "    n_clusters_ = len(set(zeroshot_labels))\n",
    "\n",
    "    print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "    print(f\"Counter zero-shot labels = {Counter(zeroshot_labels)}\")\n",
    "    # print(f\"Unique labels = {np.unique(labels)}\")\n",
    "    \n",
    "    new_docs = DocList[MultiModal](docs)\n",
    "    new_docs.zeroshot_label = zeroshot_labels\n",
    "    new_docs.zeroshot_description = [labels_prompts[i] for i in zeroshot_labels]\n",
    "    \n",
    "    return new_docs, zeroshot_labels, n_clusters_\n",
    "\n",
    "def get_zeroshot_sampling(docs: DocList[MultiModal], text_embs, n_neighbors=10):\n",
    "    \n",
    "    #1. Get the CLIP embeddings\n",
    "    x = torch.stack([doc.embedding for doc in docs])\n",
    "    \n",
    "    #2. zero-shot clustering the embeddings\n",
    "    clustered_docs, zeroshot_labels, n_clusters = zeroshot_clustering(docs, text_embs)\n",
    "\n",
    "    #3. Get stratified samples\n",
    "    sampled_docs = stratified_sampling(clustered_docs, zeroshot_labels, n_neighbors) \n",
    "\n",
    "\n",
    "    print(f\"# Sampled docs ({round((len(sampled_docs) / len(clustered_docs)) * 100, 2)}%):\", len(sampled_docs))\n",
    "    print()\n",
    "\n",
    "    return clustered_docs, sampled_docs \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566ba4cb-8c78-4ffb-95e3-50f12b34843f",
   "metadata": {},
   "source": [
    "## Pulling docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a8dcb-571c-484a-9fbb-04fe45106ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "url = 'file://./brazilian_indian'\n",
    "dl = DocList[MultiModal].pull(url)\n",
    "dl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b0172-27f3-452a-97bf-01e940f0d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Task: {Counter(dl.task)}\")\n",
    "print(f\"Task description: {Counter(dl.task_description)}\")\n",
    "\n",
    "print(f\"Label: {Counter(dl.label)}\")\n",
    "print(f\"Label description: {Counter(dl.label_description)}\")\n",
    "\n",
    "dl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47949146-3bae-41a7-a142-83b8038d2aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from docarray.data import MultiModalDataset\n",
    "\n",
    "list_doclists = list()\n",
    "unique_tasks = np.unique(dl.task)\n",
    "\n",
    "for task in unique_tasks:\n",
    "    doc_list = DocList[MultiModal]([doc for doc in dl if doc.task == task])\n",
    "    # multi_dataset = MultiModalDataset[MultiModal](doc_list, preprocessing={})\n",
    "    # processed_datasets.append(multi_dataset)\n",
    "    list_doclists.append(doc_list)\n",
    "    \n",
    "list_doclists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c7af07-c639-4c6c-8eca-b686980eb3da",
   "metadata": {},
   "source": [
    "## Visualize clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec26498-1cf9-415a-ae56-84aee4062b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "embeder, preprocess = clip.load(\"ViT-L/14@336px\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6a38cd-c233-4f03-a1fc-e973b490e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = {\n",
    "    0: 'No retinopathy', \n",
    "    1: 'Retinopathy'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943c27f9-e3be-47ae-afa7-c953d8b5a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_tokens(embeder, labels_prompts):\n",
    "\n",
    "    print(labels_prompts)\n",
    "    label_tokens = clip.tokenize(labels_prompts)\n",
    "    with torch.no_grad():\n",
    "        text_embs = embeder.encode_text(label_tokens)\n",
    "    return text_embs\n",
    "\n",
    "# text_embs = get_label_tokens(embeder, labels_prompts)\n",
    "\n",
    "# print(text_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16baa55-5207-45f6-bbe2-c6941f2759d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "description_sets_long = [\n",
    "    # Original Descriptions\n",
    "    {\n",
    "        'No retinopathy': 'No retinopathy',\n",
    "        'Retinopathy': 'Mild to Severe non-proliferative diabetic retinopathy and Proliferative diabetic retinopathy with post-laser status'\n",
    "    },\n",
    "    # Alternative 1\n",
    "    {\n",
    "        'No retinopathy': 'Normal eye fundus without signs of retinal damage',\n",
    "        'Retinopathy': 'Presence of any degree of diabetic retinopathy, including post-laser treatment'\n",
    "    },\n",
    "    # Alternative 2\n",
    "    {\n",
    "        'No retinopathy': 'Absence of retinal abnormalities associated with diabetic retinopathy',\n",
    "        'Retinopathy': 'Eye fundus showing markers of non-proliferative or proliferative diabetic retinopathy, treated or untreated'\n",
    "    },\n",
    "    # Alternative 3\n",
    "    {\n",
    "        'No retinopathy': 'Eye fundus with no diabetic retinal pathology',\n",
    "        'Retinopathy': 'Fundus image displaying features of mild to severe diabetic retinopathy, possibly with laser treatment marks'\n",
    "    },\n",
    "    # Alternative 4\n",
    "    {\n",
    "        'No retinopathy': 'Healthy retina with no evidence of retinopathy',\n",
    "        'Retinopathy': 'Retina with signs of diabetic retinopathy ranging from mild to severe, including laser-treated cases'\n",
    "    },\n",
    "    # Alternative 5\n",
    "    {\n",
    "        'No retinopathy': 'Fundus image free of diabetic retinal disease indicators',\n",
    "        'Retinopathy': 'Fundus showing varying stages of diabetic retinopathy or post-laser treatment indicators'\n",
    "    }\n",
    "]\n",
    "\n",
    "description_sets_short = [\n",
    "\n",
    "    {\n",
    "        'No retinopathy': 'Healthy retina',\n",
    "        'Retinopathy': 'Diabetic damage'\n",
    "    },\n",
    "    # Short Alternative 2\n",
    "    {\n",
    "        'No retinopathy': 'No damage',\n",
    "        'Retinopathy': 'Diabetic signs'\n",
    "    },\n",
    "    # Short Alternative 3\n",
    "    {\n",
    "        'No retinopathy': 'Normal retina',\n",
    "        'Retinopathy': 'Retinal disease'\n",
    "    },\n",
    "    # Short Alternative 4\n",
    "    {\n",
    "        'No retinopathy': 'No issues',\n",
    "        'Retinopathy': 'Retinopathy present'\n",
    "    },\n",
    "    # Short Alternative 5\n",
    "    {\n",
    "        'No retinopathy': 'Clear fundus',\n",
    "        'Retinopathy': 'Fundus changes'\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58068a56-211a-4f48-985b-f080f93d6b1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "\n",
    "\n",
    "# Templates\n",
    "templates = [\n",
    "    \"an iris with\", \n",
    "    \"a human eye with\", \n",
    "    \"\", \n",
    "    \"an ocular image with\", \n",
    "    \"a retinal photo with\",\n",
    "    \"A fundus image displaying\",\n",
    "    \"Visible symptoms in the retina suggest\",\n",
    "    \"Retinal scan reveals\",\n",
    "    \"Optical image shows\",\n",
    "    \"The condition of the retina is\"\n",
    "]\n",
    "\n",
    "task_all_accuracies = {}\n",
    "\n",
    "\n",
    "# Iterating over each template\n",
    "for task_index, task in enumerate(list_doclists):\n",
    "    all_accuracies = {}\n",
    "    all_f1_scores = {}\n",
    "    for template in templates:\n",
    "        # Initialize lists to store metrics for the current template\n",
    "        accuracies = []\n",
    "        f1_scores = []\n",
    "        # Initialize list to store description labels\n",
    "        # description_labels = []\n",
    "\n",
    "        # Iterating over each set of descriptions\n",
    "        for i, descriptions in enumerate(description_sets_short + description_sets_long):\n",
    "            description_label = f\"Set {i+1}\"\n",
    "            # description_labels.append(description_label)\n",
    "\n",
    "            labels_prompts = [f\"{template} {descriptions[c]}\" for c in class_labels.values()]\n",
    "            text_embs = get_label_tokens(embeder, labels_prompts)\n",
    "\n",
    "            clustered_docs, sampled_docs = get_zeroshot_sampling(task, text_embs, n_neighbors=10)\n",
    "            ground_truth_labels = clustered_docs.label\n",
    "            zeroshot_labels = clustered_docs.zeroshot_label\n",
    "\n",
    "            accuracy = metrics.accuracy_score(ground_truth_labels, zeroshot_labels)\n",
    "            f1 = metrics.f1_score(ground_truth_labels, zeroshot_labels, average='weighted')\n",
    "\n",
    "            accuracies.append(accuracy)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "        all_accuracies[template] = accuracies\n",
    "        all_f1_scores[template] = f1_scores\n",
    "\n",
    "    task_all_accuracies[task_index] = {\"accuracy\": all_accuracies, \"f1-score\": all_f1_scores}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bac69f-811d-4fab-bcc9-237999786165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined list of all description sets (both short and long)\n",
    "all_description_sets = description_sets_short + description_sets_long\n",
    "\n",
    "# Iterate through each template\n",
    "score = \"f1-score\"\n",
    "best_values = dict()\n",
    "\n",
    "for task, value in task_all_accuracies.items():\n",
    "    best_template = None\n",
    "    best_description_set = None\n",
    "    best_score = 0\n",
    "\n",
    "    print(\"Task: \", task)\n",
    "    for template, accuracies in value[score].items():\n",
    "        # Iterate through each description set\n",
    "        for i, accuracy in enumerate(accuracies):\n",
    "            if accuracy > best_score:\n",
    "                best_score = accuracy\n",
    "                best_template = template\n",
    "                best_description_set = all_description_sets[i]\n",
    "\n",
    "    best_values[task] = {\"best_score\": best_score, \"best_template\": best_template, \"best_description_set\": best_description_set}\n",
    "    print(\"Best Template:\", best_template)\n",
    "    print(\"Best Description Set:\", best_description_set)\n",
    "    print(\"Best F1-Score:\", best_score)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405bf081-02ac-4ca2-930b-3b1a5474e8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_prompts_best = [f\"{best_template} {best_description_set[c]}\" for c in class_labels.values()]\n",
    "text_embs_best = get_label_tokens(embeder, labels_prompts_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a317a968-2656-4173-b378-45e1b2bbd48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7174edc9-f932-4b57-ab63-14651c3758c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize f1_matrix\n",
    "\n",
    "for task_index, task in enumerate(list_doclists):\n",
    "    all_f1_scores_ = task_all_accuracies[task_index]['f1-score']\n",
    "    num_templates = len(templates)\n",
    "    num_description_sets = len(description_sets_short) + len(description_sets_long)\n",
    "    f1_matrix = np.zeros((num_templates, num_description_sets))\n",
    "\n",
    "    # Fill in f1_matrix with the values from all_f1_scores\n",
    "    for i, template in enumerate(templates):\n",
    "        for j in range(num_description_sets):\n",
    "            f1_matrix[i, j] = all_f1_scores_[template][j]\n",
    "\n",
    "    # Plotting the F1-Score matrix\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "    cax = ax.matshow(f1_matrix, cmap='coolwarm')\n",
    "\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes explicitly\n",
    "    ax.set_xticks(np.arange(num_description_sets))\n",
    "    ax.set_xticklabels([f\"Set {i+1}\" for i in range(num_description_sets)], rotation=45)\n",
    "    ax.set_yticks(np.arange(num_templates))\n",
    "    ax.set_yticklabels([t if t != '' else '[No template]' for t in templates])\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(f1_matrix.shape[0]):\n",
    "        for j in range(f1_matrix.shape[1]):\n",
    "            ax.text(j, i, f\"{f1_matrix[i, j]:.3f}\", ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    plt.xlabel('Description Sets')\n",
    "    plt.ylabel('Templates')\n",
    "    plt.title(f'F1-Scores for Task {task_index}')\n",
    "    plt.savefig(f'ConfusionMatrix_{task_index}.pdf', bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d722bd4-4170-45e4-b18f-9f68b3cf20d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "# Initialize UMAP\n",
    "umap_model = umap.UMAP(n_neighbors=20, n_components=2, min_dist=0.1, metric='cosine')\n",
    "\n",
    "# Assuming you already have your multi-modal data and text embeddings\n",
    "\n",
    "for i, docs in enumerate(list_doclists):\n",
    "    print(f\">>>> TASK = {i}\")\n",
    "    clustered_docs, sampled_docs = get_zeroshot_sampling(docs, text_embs_best, n_neighbors=10)\n",
    "    ground_truth_labels = clustered_docs.label\n",
    "    zeroshot_labels = clustered_docs.zeroshot_label\n",
    "\n",
    "    print(f\"Accuracy: {metrics.accuracy_score(ground_truth_labels, zeroshot_labels):.3f}\")\n",
    "    print(f\"f1_score: {metrics.f1_score(ground_truth_labels, zeroshot_labels, average='weighted'):.3f}\")  \n",
    "\n",
    "    # Assume the embeddings are stored in a 'features' attribute\n",
    "    docs_embeddings = torch.stack([data.embedding for data in clustered_docs]).detach().cpu().numpy()\n",
    "    sampled_embeddings = torch.stack([doc.embedding for doc in sampled_docs]).detach().cpu().numpy()\n",
    "\n",
    "    # Pre-compute 2D UMAP for visualization\n",
    "    docs_embeddings_2d = umap_model.fit_transform(docs_embeddings)\n",
    "    sampled_embeddings_2d = umap_model.transform(sampled_embeddings)  # You could save this and only load when needed\n",
    "\n",
    "    # Define Colors\n",
    "    cluster_colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, 2+1)]\n",
    "    cluster_colors_ = [cluster_colors[i] for i in ground_truth_labels]\n",
    "    sampled_colors_ = [cluster_colors[i] for i in sampled_docs.zeroshot_label]\n",
    "\n",
    "    # Fast plotting\n",
    "    plt.scatter(docs_embeddings_2d[:, 0], docs_embeddings_2d[:, 1], c=cluster_colors_, marker='o', s=300, alpha=0.2, label=f'Clusters')\n",
    "    plt.scatter(sampled_embeddings_2d[:, 0], sampled_embeddings_2d[:, 1], c=sampled_colors_, marker='*', s=150, label=f'Zero-shot sampling', edgecolors=\"black\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(f\"UMAP Projection for Domain = {i}\")\n",
    "    # plt.savefig(f'projection_{i}.pdf', bbox_inches='tight', pad_inches=0)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7993d27f-1ac1-4d1b-9db3-a968f4e09cfa",
   "metadata": {},
   "source": [
    "## TADILER: TADIL + Experience Replay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee2146d-7422-458a-a3b9-c48eaeaa0cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avalanche.training.storage_policy import ParametricBuffer, _ParametricSingleBuffer\n",
    "from avalanche.benchmarks.utils.data_loader import ReplayDataLoader\n",
    "from avalanche.training.plugins import SupervisedPlugin\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class RandomExemplarsBuffer(ParametricBuffer):\n",
    "    def __init__(self, max_size, n_neighbors=10, seed=42, groupby=None, \n",
    "                 selection_strategy=None):\n",
    "        super().__init__(max_size, groupby, selection_strategy)\n",
    "        self.n_neighbors = n_neighbors\n",
    "        print(\">>>>RandomExemplarsBuffer\")\n",
    "\n",
    "    def update(self, strategy, **kwargs):\n",
    "        dataset_len = len(strategy.experience.dataset)\n",
    "        if self.n_neighbors > dataset_len:\n",
    "            raise ValueError(\"n_neighbors cannot be greater than the size of the dataset\")\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        random_indices = np.random.choice(dataset_len, self.n_neighbors, replace=False)\n",
    "        \n",
    "        x_random = [strategy.experience.dataset[i][0] for i in random_indices]\n",
    "        # y_random = [1 for i in random_indices]\n",
    "        y_random = [strategy.experience.dataset[i][1] for i in random_indices]\n",
    "        # t_random = [0 for i in random_indices]\n",
    "        t_random = [strategy.experience.dataset[i][2] for i in random_indices]\n",
    "\n",
    "        x_random_pt = torch.stack(x_random)\n",
    "        y_random_pt = torch.from_numpy(np.asarray(y_random))\n",
    "        t_random_pt = torch.from_numpy(np.asarray(t_random))\n",
    "\n",
    "        # include task labels when creating new_data\n",
    "\n",
    "\n",
    "        # new_data = TensorDataset(x_random_pt, y_random_pt)\n",
    "        new_data = TensorDataset(x_random_pt, y_random_pt, t_random_pt)\n",
    "        new_data = AvalancheDataset(new_data)\n",
    "        new_data.targets = y_random_pt.tolist()\n",
    "        # new_data = make_classification_dataset(new_data, task_labels=t_random_pt)\n",
    "        \n",
    "        print(\"Update x_random: \", len(x_random))\n",
    "        print(\"Update y_random: \",  len(y_random))\n",
    "        print(\"Update new_data: \", len(new_data))\n",
    "        print(\"Update new_data[0]: \", len(new_data[0]))\n",
    "        \n",
    "        new_groups = self._make_groups(strategy, new_data)\n",
    "        self.seen_groups.update(new_groups.keys())\n",
    "\n",
    "        lens = self.get_group_lengths(len(self.seen_groups))\n",
    "        group_to_len = {group_id: ll for group_id, ll in zip(self.seen_groups, lens)}\n",
    "\n",
    "        for group_id, new_data_g in new_groups.items():\n",
    "            ll = group_to_len[group_id]\n",
    "            if group_id in self.buffer_groups:\n",
    "                old_buffer_g = self.buffer_groups[group_id]\n",
    "                old_buffer_g.update_from_dataset(strategy, new_data_g)\n",
    "                old_buffer_g.resize(strategy, ll)\n",
    "            else:\n",
    "                new_buffer = _ParametricSingleBuffer(ll, self.selection_strategy)\n",
    "                new_buffer.update_from_dataset(strategy, new_data_g)\n",
    "                self.buffer_groups[group_id] = new_buffer\n",
    "\n",
    "        for group_id, class_buf in self.buffer_groups.items():\n",
    "            self.buffer_groups[group_id].resize(strategy, group_to_len[group_id])\n",
    "                      \n",
    "\n",
    "class ZeroshotExemplarsBuffer(ParametricBuffer):\n",
    "    def __init__(self, max_size, n_neighbors=10, groupby=None, \n",
    "                 selection_strategy=None):\n",
    "        super().__init__(max_size, groupby, selection_strategy)\n",
    "        self.n_neighbors = n_neighbors\n",
    "        print(\">>>>ZeroshotExemplarsBuffer\")\n",
    "        \n",
    "    def update(self, strategy, **kwargs):\n",
    "        data_list = list()\n",
    "        for data in strategy.experience.dataset:\n",
    "            data_list.append(MultiModal(embedding=data[0], path=\"\", label=data[1], label_description=\"\",\n",
    "                                        zeroshot_label=-1, zeroshot_description=\"\",\n",
    "                                            task=data[2], task_description=\"\",\n",
    "                                            id_code=\"\", metadata={}))\n",
    "        multimodal_data = DocList[MultiModal](data_list)\n",
    "        print(type(multimodal_data))\n",
    "        clustered_docs, sampled_docs = get_zeroshot_sampling(multimodal_data, text_embs_best, n_neighbors=self.n_neighbors)\n",
    "        \n",
    "        x_knn = sampled_docs.embedding\n",
    "        y_knn = sampled_docs.label\n",
    "        t_knn = sampled_docs.task\n",
    "        # x_knn, knn_indices = get_zeroshot_sampling(strategy.experience.dataset, self.n_neighbors)\n",
    "        # y_knn = [strategy.experience.dataset[i][1] for i in knn_indices]\n",
    "        # t_knn = [strategy.experience.dataset[i][2] for i in knn_indices]\n",
    "        \n",
    "        x_knn_pt = torch.stack(x_knn)\n",
    "        # x_knn_pt = torch.from_numpy(x_knn)\n",
    "        y_knn_pt = torch.from_numpy(np.asarray(y_knn))\n",
    "        t_knn_pt = torch.from_numpy(np.asarray(t_knn))\n",
    "        \n",
    "        # include task labels when creating new_data\n",
    "        # new_data = TensorDataset(x_knn_pt, y_knn_pt)\n",
    "        new_data = TensorDataset(x_knn_pt, y_knn_pt, t_knn_pt)\n",
    "        new_data = AvalancheDataset(new_data)\n",
    "        new_data.targets = y_knn_pt.tolist()\n",
    "        # new_data = make_classification_dataset(new_data, task_labels=y_knn_pt)\n",
    "\n",
    "        # print(\"@\"*50)\n",
    "        print(\"Update x_knn: \", len(x_knn))\n",
    "        print(\"Update y_knn: \",  len(y_knn))\n",
    "        print(\"Update new_data: \", len(new_data))\n",
    "        print(\"Update new_data[0] before: \", len(new_data[0]))\n",
    "        \n",
    "        new_groups = self._make_groups(strategy, new_data)\n",
    "        self.seen_groups.update(new_groups.keys())\n",
    "\n",
    "        lens = self.get_group_lengths(len(self.seen_groups))\n",
    "        group_to_len = {group_id: ll for group_id, ll in zip(self.seen_groups, lens)}\n",
    "\n",
    "        for group_id, new_data_g in new_groups.items():\n",
    "            ll = group_to_len[group_id]\n",
    "            if group_id in self.buffer_groups:\n",
    "                old_buffer_g = self.buffer_groups[group_id]\n",
    "                old_buffer_g.update_from_dataset(strategy, new_data_g)\n",
    "                old_buffer_g.resize(strategy, ll)\n",
    "            else:\n",
    "                new_buffer = _ParametricSingleBuffer(ll, self.selection_strategy)\n",
    "                new_buffer.update_from_dataset(strategy, new_data_g)\n",
    "                self.buffer_groups[group_id] = new_buffer\n",
    "\n",
    "        for group_id, class_buf in self.buffer_groups.items():\n",
    "            self.buffer_groups[group_id].resize(strategy, group_to_len[group_id])\n",
    "        print(\"Update new_data[0] after: \", len(new_data[0]))\n",
    "\n",
    "\n",
    "class TADILER(SupervisedPlugin):\n",
    "    def __init__(self, storage_policy):\n",
    "        super().__init__()\n",
    "        self.storage_policy = storage_policy\n",
    "        # print(\"TADILER Wrapper\")\n",
    "\n",
    "    def before_training_exp(self, strategy,\n",
    "                            num_workers: int = 0, shuffle: bool = False,\n",
    "                            **kwargs):\n",
    "        \n",
    "        # print(dir(strategy))\n",
    "        if len(self.storage_policy.buffer) == 0:\n",
    "            return\n",
    "\n",
    "        # print(\"@\"*50)\n",
    "        print(f\"strategy.adapted_dataset: {len(strategy.adapted_dataset[0])}, Length: {len(strategy.adapted_dataset)}\")\n",
    "        print(f\"self.storage_policy.buffer: {len(self.storage_policy.buffer[0])}, Length: {len(self.storage_policy.buffer)}\")\n",
    "        print(f\"strategy.train_mb_size: {strategy.train_mb_size}\")\n",
    "        \n",
    "        strategy.dataloader = ReplayDataLoader(\n",
    "            strategy.adapted_dataset,\n",
    "            self.storage_policy.buffer,\n",
    "            # oversample_small_tasks=True,\n",
    "            # task_balanced_dataloader=True,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=strategy.train_mb_size,\n",
    "            shuffle=shuffle)\n",
    "        \n",
    "    def after_training_exp(self, strategy, **kwargs):\n",
    "        self.storage_policy.update(strategy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99248c9-3fd2-4ab7-b1c8-bdbb58b87200",
   "metadata": {},
   "source": [
    "### Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30a0fa3-cd7a-49b6-9f2b-25c83147cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from random import shuffle, seed\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Dataset generator for the Custom dataset\"\"\"\n",
    "\n",
    "    def __init__(self, docs: DocList[MultiModal], transform=None):\n",
    "        self.docs = docs\n",
    "        self.transform = transform\n",
    "        self.targets = [doc.label for doc in self.docs]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the size of the dataset\"\"\"\n",
    "        return len(self.docs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns a batch of image, labels as Torch tensors\"\"\"\n",
    "        image = self.docs[idx].embedding\n",
    "        label = self.docs[idx].label\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a6d9a5-f251-4ac8-8793-5a5a2d300855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docarray import DocVec\n",
    "DocVec[MultiModal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c63f1e5-83d1-461c-803a-d6fa8ed401f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle, seed\n",
    "\n",
    "def split_datasets(docs: DocList[MultiModal], seed_val=123, train_percentage=0.8):\n",
    "\n",
    "    shuffled_docs = list(docs)\n",
    "    seed(seed_val)\n",
    "    shuffle(shuffled_docs)\n",
    "    shuffled_docs = DocList[MultiModal](shuffled_docs)\n",
    "\n",
    "    split_idx = int(train_percentage * len(shuffled_docs))\n",
    "    train_docs = shuffled_docs[:split_idx]\n",
    "    test_docs = shuffled_docs[split_idx:]\n",
    "\n",
    "\n",
    "    train_dataset = CustomDataset(train_docs)\n",
    "    test_dataset = CustomDataset(test_docs)\n",
    "\n",
    "    return train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db0622b-f06b-4b6d-88a6-4ed83fabad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Define the split sizes for each dataset\n",
    "train_size = 0.8\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "train_sets_method = []\n",
    "test_sets_method = []\n",
    "for doclist in list_doclists:\n",
    "\n",
    "    train_dataset, test_dataset = split_datasets(doclist, seed_val=SEED, train_percentage=train_size)\n",
    "\n",
    "\n",
    "    train_sets_method.append(train_dataset)\n",
    "    test_sets_method.append(test_dataset)\n",
    "\n",
    "print(\"#Method data\")\n",
    "print('Training lenghts: ', [len(ts) for ts in train_sets_method])\n",
    "print('Testing lenghts: ', [len(ts) for ts in test_sets_method])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6d2f0e-d196-42b6-ae85-c2d5909f873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(train_sets_method[1].docs.label_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540bc193-c032-48ba-a6d0-b056afd60876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from avalanche.benchmarks.utils import AvalancheDataset\n",
    "from avalanche.benchmarks.utils import make_classification_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from avalanche.benchmarks.generators import filelist_benchmark, dataset_benchmark\n",
    "from torch.utils.data import TensorDataset\n",
    "    \n",
    "# Method benchmark\n",
    "training_datasets = list()\n",
    "testing_datasets = list()\n",
    "\n",
    "for task, (train_s, test_s) in enumerate(zip(train_sets_method, test_sets_method), start=0):\n",
    "\n",
    "    training_datasets.append(make_classification_dataset(train_s, task_labels=train_s.docs.task))\n",
    "    testing_datasets.append(make_classification_dataset(test_s, task_labels=test_s.docs.task))\n",
    "\n",
    "    \n",
    "benchmark_method = dataset_benchmark(\n",
    "    train_datasets=training_datasets,\n",
    "    test_datasets=testing_datasets,\n",
    "    # other_streams_datasets={'metadata': make_classification_dataset(dataset=[e.id_code for e in train_s], targets=[e.label for e in train_s])}\n",
    "    # other_streams_datasets={'multimodal_train': make_classification_dataset(dataset=[e.id_code for e in train_s], targets=[e.label for e in train_s])}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602fb01c-3fdf-43c9-aa5a-e8928df1b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique(downsampled_data['DR_ICDR'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d57375-8a71-437b-91c1-1803e50a2838",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_method.task_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee7eddd-cf1f-4c07-9eac-9527070bdc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence(sequence, stream):\n",
    "    return [stream[i] for i in sequence]\n",
    "sequence = [0, 1]\n",
    "stream_seq = get_sequence(sequence, benchmark_method.train_stream)\n",
    "stream_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875cf4df-0e78-40b7-a4ab-82a9230b9e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.all(stream_seq[0].dataset[0][0] == stream_seq[1].dataset[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1388e7-a18f-4e8c-b256-f453fbd990b4",
   "metadata": {},
   "source": [
    "## Prepare the strategies to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be746403-69a5-4b81-b610-e5a580b3b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_strategy(strategy_name, model, optimizer, criterion, eval_plugin, n_epochs, custom_replay):\n",
    "    \n",
    "    strategies = {\n",
    "        'Naive': Naive( #Regularization-based method\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            train_mb_size=200,\n",
    "            train_epochs=n_epochs,\n",
    "            eval_mb_size=200,\n",
    "            device=device,\n",
    "            evaluator=eval_plugin,\n",
    "            plugins=[custom_replay],\n",
    "        ),\n",
    "        'EWC': EWC( #Regularization-based method\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            train_mb_size=200,\n",
    "            train_epochs=n_epochs,\n",
    "            eval_mb_size=200,\n",
    "            device=device,\n",
    "            evaluator=eval_plugin,\n",
    "            ewc_lambda=0.2,\n",
    "            plugins=[custom_replay],\n",
    "        ),\n",
    "        'Replay': Replay( #Rehearsal-based method\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            train_mb_size=200,\n",
    "            train_epochs=n_epochs,\n",
    "            eval_mb_size=200,\n",
    "            device=device,\n",
    "            evaluator=eval_plugin,\n",
    "            plugins=[custom_replay],\n",
    "            mem_size=20,\n",
    "        ),\n",
    "        'LwF': LwF( #Architecture-based method\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            alpha=0.5,\n",
    "            temperature=0.2,\n",
    "            train_epochs=n_epochs,\n",
    "            device=device,\n",
    "            train_mb_size=200,\n",
    "            eval_mb_size=200,\n",
    "            evaluator=eval_plugin,\n",
    "            plugins=[custom_replay],\n",
    "        ),\n",
    "        'GEM': GEM( #Architecture-based method\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            # alpha=0.5,\n",
    "            # temperature=0.2,\n",
    "            patterns_per_exp=10,\n",
    "            train_epochs=n_epochs,\n",
    "            device=device,\n",
    "            train_mb_size=200,\n",
    "            eval_mb_size=200,\n",
    "            evaluator=eval_plugin,\n",
    "            plugins=[custom_replay],\n",
    "        ),\n",
    "\n",
    "    }\n",
    "\n",
    "    return strategies[strategy_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430378d6-2741-43d9-9c0c-fc184ef91746",
   "metadata": {},
   "source": [
    "## Prepare and run the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7de4d4-c8ff-4f2f-94d3-748c3a9d037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_exp_name = 'Agnostic_no_repetition_improver'\n",
    "# root_exp_name = 'Agnostic_with_repetition'\n",
    "# sequence = [0, 1, 2, 1, 3, 3, 4, 5]\n",
    "sequence = list(range(len(benchmark_method.train_stream)))\n",
    "print(f\"Sequence = {sequence}\")\n",
    "n_epochs = 6\n",
    "\n",
    "# n_neighbors = 50\n",
    "# list_n_neighbors = [1, 5, 10, 15]\n",
    "# list_n_neighbors = [15, 20]\n",
    "list_n_neighbors = [15, 20, 25, 30, 50]\n",
    "# list_n_neighbors = [25, 30, 50]\n",
    "\n",
    "# list_n_neighbors = [1]\n",
    "# random_seeds = [23, 24, 25]\n",
    "# random_seeds = [34, 88, 100]\n",
    "# random_seeds = [3]\n",
    "random_seeds = [3, 11, 51]\n",
    "\n",
    "num_experiments = len(random_seeds)\n",
    "print(f\"n_exps = {num_experiments}\")\n",
    "# print(f\"Setup = {root_exp_name}\")\n",
    "print(f\"Epochs = {n_epochs}\")\n",
    "print(f\"Seeds = {random_seeds}\")\n",
    "print(f\"list n_neighbors = {list_n_neighbors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da5cae4-a742-468b-86cb-a6d90dfe5dc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "import torch\n",
    "import copy\n",
    "import random\n",
    "# import wandb\n",
    "import gc\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import copy\n",
    "import statistics as st\n",
    "from torch.optim import Adam\n",
    "from rich import print\n",
    "from avalanche.models import as_multitask\n",
    "from avalanche.benchmarks.classic import SplitMNIST, PermutedMNIST\n",
    "from avalanche.models import MTSimpleMLP, MTSimpleCNN, SimpleCNN, SimpleMLP\n",
    "# from avalanche.training.supervised import EWC, Naive, LFL\n",
    "from avalanche.evaluation.metrics import (\n",
    "    forgetting_metrics,\n",
    "    accuracy_metrics,\n",
    "    confusion_matrix_metrics\n",
    ")\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger, WandBLogger\n",
    "# from avalanche.training import Naive\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "from avalanche.benchmarks.classic import SplitMNIST, PermutedMNIST, RotatedMNIST, SplitCIFAR10, SplitCIFAR100\n",
    "from avalanche.benchmarks.generators import filelist_benchmark, dataset_benchmark\n",
    "from avalanche.training.utils import adapt_classification_layer\n",
    "\n",
    "from datetime import datetime\n",
    "from avalanche.evaluation.metrics.images_samples import images_samples_metrics\n",
    "from avalanche.evaluation.metrics.labels_repartition import (\n",
    "    labels_repartition_metrics,\n",
    ")\n",
    "from avalanche.evaluation.metrics.mean_scores import mean_scores_metrics\n",
    "from torch.nn import Linear\n",
    "from avalanche.training.supervised import Replay, GEM, LwF, EWC, Naive\n",
    "\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "\n",
    "for exp in range(num_experiments):\n",
    "\n",
    "    for n_neighbors in list_n_neighbors:\n",
    "        print(f\"Length training: {len(benchmark_method.train_stream[0].dataset)}\")\n",
    "        print(f\"Length testing: {len(benchmark_method.test_stream[0].dataset)}\")\n",
    "\n",
    "        print(\"-\"*50)\n",
    "\n",
    "        n_workers = 8\n",
    "        device = 'cpu'\n",
    "        seed = random_seeds[exp]\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        # print(f\"Scenario = {root_exp_name}\")\n",
    "        print(f\"Experiment: {exp+1} / {num_experiments}\")\n",
    "        print(f\"Seed = {seed}\")\n",
    "        print(f\"n_epochs = {n_epochs}\")\n",
    "        print(f\"n_neighbors = {n_neighbors}\")\n",
    "        # torch.cuda.manual_seed(seed)\n",
    "        # all_strategies = ['TADILER']\n",
    "        # all_strategies = ['Naive']\n",
    "        n_classes = len(np.unique(dl.label))\n",
    "        embedding_shape = dl[0].embedding.shape[0]\n",
    "        # all_strategies = ['Replay_Random', 'TADILER']\n",
    "        # all_strategies = ['Naive']\n",
    "        all_strategies = ['Naive', 'GEM', 'LwF', 'EWC']\n",
    "        # all_strategies = ['EWC', 'LwF']\n",
    "        # all_strategies = ['EWC','Replay', 'LwF', 'TADILER', 'Replay_Random', 'Naive']\n",
    "\n",
    "        for index_strat, strategy_name in enumerate(all_strategies):\n",
    "\n",
    "            selection_strategy_random = TADILER(RandomExemplarsBuffer(max_size=2000, groupby=None, \n",
    "                                                               n_neighbors=n_classes*n_neighbors))\n",
    "            selection_strategy_tadiler = TADILER(ZeroshotExemplarsBuffer(max_size=2000, groupby=None, \n",
    "                                                            n_neighbors=n_neighbors))\n",
    "            \n",
    "            dict_selection_strategy = {'Random': selection_strategy_random, \n",
    "                                       'TADILER': selection_strategy_tadiler}\n",
    "            \n",
    "            for index_select, (name_selection, selection_strategy) in enumerate(dict_selection_strategy.items()):\n",
    "                benchmark_method_train_stream = get_sequence(sequence, benchmark_method.train_stream)\n",
    "                benchmark_method_test_stream = get_sequence(sequence, benchmark_method.test_stream)\n",
    "\n",
    "                loggers = []\n",
    "                # wandb_logger = WandBLogger(\n",
    "                #     project_name=\"TADILER\", run_name=f\"strategy-{strategy_name}-knn-{n_neighbors}-seed-{seed}\"\n",
    "                # )\n",
    "                loggers.append(InteractiveLogger())\n",
    "                # loggers.append(wandb_logger)\n",
    "\n",
    "                eval_plugin = EvaluationPlugin(\n",
    "                    accuracy_metrics(\n",
    "                        minibatch=False, epoch=False, experience=True\n",
    "                    ),\n",
    "\n",
    "                    forgetting_metrics(experience=True),\n",
    "                    confusion_matrix_metrics(save_image=False, normalize=None, stream=True),\n",
    "                    loggers=loggers,\n",
    "                )\n",
    "                \n",
    "                n_inputs = embedding_shape  # input size after CLIP embedding\n",
    "                n_outputs = n_classes \n",
    "                hidden_layer_size = 256 \n",
    "\n",
    "                # Define the model\n",
    "                model = Sequential(\n",
    "                    Linear(n_inputs, hidden_layer_size),\n",
    "                    ReLU(),\n",
    "                    Linear(hidden_layer_size, n_outputs)\n",
    "                )\n",
    "\n",
    "                mt_model = as_multitask(model, '2')  # modify 'fc' with the index or name of your last layer\n",
    "\n",
    "                optimizer = Adam(mt_model.parameters(), lr=0.001)\n",
    "                criterion = CrossEntropyLoss()\n",
    "                # criterion = CrossEntropyLoss(weight=class_weights)\n",
    "        \n",
    "                strategy = get_strategy(strategy_name, mt_model, optimizer, criterion, \n",
    "                                        eval_plugin, n_epochs, selection_strategy)\n",
    "\n",
    "                print(f\"Running strategy: {strategy_name}-{name_selection}, experiment: {exp}, n_neighbors={n_neighbors}\")\n",
    "                results = {key: [] for key in ['values']}\n",
    "\n",
    "\n",
    "                print(f\"Sequence: {sequence}\")\n",
    "                for index, (task, experience) in enumerate(zip(sequence, benchmark_method_train_stream)):\n",
    "\n",
    "                    curr_experience = experience.current_experience\n",
    "                    print(\"Current experience: \", curr_experience)    \n",
    "                    print(\"Experience task ID \", experience.task_label)\n",
    "                    print('Experience shape:', len(experience.dataset))            \n",
    "\n",
    "                    print(\"Training multi-head model\")\n",
    "                    # print(strategy)\n",
    "                    strategy.train(experience, num_workers=n_workers)\n",
    "                    print('Training multi-head model completed')            \n",
    "\n",
    "                    print(f\"Evaluation benchmark: {strategy_name}-{name_selection}\")\n",
    "\n",
    "                    values = strategy.eval(benchmark_method_test_stream[:index+1], num_workers=n_workers)\n",
    "\n",
    "                    if index == 0 and index_select == 0:\n",
    "                        values_initial = values\n",
    "                    elif index == 0 and index_select == 1:\n",
    "                        values = values_initial\n",
    "\n",
    "                    results['values'].append(values)\n",
    "\n",
    "                    print(\"@\"*60)\n",
    "                    print()\n",
    "\n",
    "                with open(f'./results/{strategy_name}-{name_selection}_knn_{n_neighbors}_seed_{seed}.pickle', 'wb') as handle:\n",
    "                    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                    print(f\"Saved results for {root_exp_name}_{strategy_name}_{name_selection}, seed: {seed}\")\n",
    "\n",
    "                print(\"&\"*100)\n",
    "                print()\n",
    "        print(\"#\"*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d492b37a-ca30-4f8e-a0db-e663def80242",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"List of n_neighbors: {list_n_neighbors}\")\n",
    "print(f\"Seeds: {random_seeds}\")\n",
    "print(f\"Strategies: {all_strategies}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453790ce-3f2e-4d3f-86d0-1770ff3649e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It takes 40min to run all the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdd39d0-e570-42df-8726-e373200176f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from rich import print\n",
    "\n",
    "global_avg_acc = dict() # Final Table\n",
    "s_name = \"Naive\"\n",
    "# n_neighbors = \"*\"\n",
    "n_neighbors = 25\n",
    "seed = \"3\"\n",
    "# seed = 3 #*\n",
    "# 51, 42, 7\n",
    "files = glob.glob(f\"./results/{s_name}*_*knn_{n_neighbors}_seed_{seed}.pickle\")\n",
    "# files = glob.glob(f\"./results/{root_exp_name}*_*knn_{n_neighbors}_seed_{seed}.pickle\")\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da63c6ba-233f-476e-82ce-63f421d29aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "def amca_from_confusion_matrix(confusion_matrix):\n",
    "    # Convert tensor to numpy array\n",
    "    confusion_matrix = confusion_matrix.numpy()\n",
    "    # Calculate per-class accuracy (diagonal elements / sum of row elements)\n",
    "    class_accuracies = np.diagonal(confusion_matrix) / np.sum(confusion_matrix, axis=1)\n",
    "    # Return the average of class accuracies\n",
    "    return np.nanmean(class_accuracies)\n",
    "\n",
    "def weighted_f1_from_confusion_matrix(confusion_matrix):\n",
    "    # Convert tensor to numpy array if it's a tensor\n",
    "    if hasattr(confusion_matrix, 'numpy'):\n",
    "        confusion_matrix = confusion_matrix.numpy()\n",
    "    \n",
    "    # Calculate precision for each class: TP / (TP + FP)\n",
    "    # TP is the diagonal element and FP is the sum of column excluding the diagonal\n",
    "    precision = np.diagonal(confusion_matrix) / np.sum(confusion_matrix, axis=0)\n",
    "    \n",
    "    # Calculate recall for each class: TP / (TP + FN)\n",
    "    # TP is the diagonal element and FN is the sum of row excluding the diagonal\n",
    "    recall = np.diagonal(confusion_matrix) / np.sum(confusion_matrix, axis=1)\n",
    "    \n",
    "    # Calculate F1-score for each class: 2 * (precision * recall) / (precision + recall)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    # Calculate the weighted F1-score\n",
    "    weights = np.sum(confusion_matrix, axis=1) / np.sum(confusion_matrix)\n",
    "    weighted_f1 = np.nansum(f1_scores * weights)\n",
    "    \n",
    "    return weighted_f1\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "for f in files:\n",
    "    # Get the method and seed from the filename\n",
    "    method_seed = f.split('_')[-5:]\n",
    "    method = method_seed[0]\n",
    "    seed = method_seed[4].split('.')[0]\n",
    "    knn = method_seed[2]\n",
    "\n",
    "    # Load the results\n",
    "    with open(f, 'rb') as handle:\n",
    "        r = pickle.load(handle)\n",
    "\n",
    "    data = r['values']\n",
    "    performances = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        # Calculate the AMCA of the current task\n",
    "        current_task_performance = amca_from_confusion_matrix(data[i]['ConfusionMatrix_Stream/eval_phase/test_stream'])\n",
    "        performances.append(current_task_performance)\n",
    "\n",
    "    # If the method is not in the dictionary, add it\n",
    "    if method not in data_dict:\n",
    "        data_dict[method] = {}\n",
    "\n",
    "    # Add the performances to the data dictionary\n",
    "    data_dict[method][seed] = performances\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for method, seeds in data_dict.items():\n",
    "    # Get all the performances for this method\n",
    "    all_performances = np.array(list(seeds.values()))\n",
    "\n",
    "    # Calculate the mean, min, and max performances for each task\n",
    "    mean_performances = np.mean(all_performances, axis=0)\n",
    "    print(f'Method: {method} | Values: {mean_performances}')\n",
    "    min_performances = np.min(all_performances, axis=0)\n",
    "    max_performances = np.max(all_performances, axis=0)\n",
    "\n",
    "    # Calculate overall average performance\n",
    "    overall_avg_performance = np.mean(mean_performances)\n",
    "\n",
    "    # Generate task numbers\n",
    "    tasks = range(1, len(mean_performances) + 1)\n",
    "\n",
    "    # Plot the mean performances and fill between the min and max performances\n",
    "    if 'Tadiler' in method:\n",
    "        plt.plot(tasks, mean_performances, marker='*', linestyle='-', color='#8B0000',\n",
    "                 label=f'{method} (avg: {overall_avg_performance:.3f})', linewidth=2)\n",
    "        plt.fill_between(tasks, min_performances, max_performances, color='#8B0000', alpha=0.2)\n",
    "    else:\n",
    "        plt.plot(tasks, mean_performances, marker='o', linestyle='-',\n",
    "                 label=f'{method} (avg: {overall_avg_performance:.3f})')\n",
    "        plt.fill_between(tasks, min_performances, max_performances, alpha=0.2)\n",
    "    # plt.fill_between(tasks, min_performances, max_performances, alpha=0.2)\n",
    "\n",
    "plt.xlabel('Task Number')\n",
    "plt.ylabel('AMCA')\n",
    "plt.title(f'AMCA over Tasks, n_samples={knn}')\n",
    "plt.grid(True)\n",
    "plt.xticks(tasks)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab94630-784d-4095-8efc-e5bdb043a19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_avg_acc = dict() # Final Table\n",
    "# all_strategies = ['LwF', 'EWC']\n",
    "all_strategies = ['Naive','GEM', 'LwF', 'EWC']\n",
    "\n",
    "for strategy_name in all_strategies:\n",
    "    # root_exp_name = f'Agnostic_no_repetition_improver_{strategy_name}'\n",
    "    n_neighbors = \"*\"\n",
    "    # n_neighbors = 1\n",
    "    seed = \"*\"\n",
    "    # seed = \"11\"\n",
    "    # 51, 42, 7\n",
    "    files = glob.glob(f\"./results/{strategy_name}*_*knn_{n_neighbors}_seed_{seed}.pickle\")\n",
    "    # files = [f for f in files if \"seed_3\" in f]\n",
    "    files = [f for f in files if \"seed_3\" in f or \"seed_11\" in f]\n",
    "    # files = [f for f in files if \"seed_11\" in f or \"seed_7.\" in f]\n",
    "    # print(files)\n",
    "\n",
    "    data_dict = {}\n",
    "\n",
    "    for f in files:\n",
    "\n",
    "        method_seed = f.split('_')[-5:]\n",
    "        method = method_seed[0]\n",
    "        seed = method_seed[4].split('.')[0]\n",
    "        knn = int(method_seed[2])\n",
    "\n",
    "        # Load the results\n",
    "        with open(f, 'rb') as handle:\n",
    "            r = pickle.load(handle)\n",
    "\n",
    "        data = r['values']\n",
    "        # task_amcas = [weighted_f1_from_confusion_matrix(d['ConfusionMatrix_Stream/eval_phase/test_stream']) for d in data]\n",
    "        task_amcas = [amca_from_confusion_matrix(d['ConfusionMatrix_Stream/eval_phase/test_stream']) for d in data]\n",
    "        overall_amca = np.mean(task_amcas)  # Average of AMCAs for all tasks\n",
    "\n",
    "        # If the method is not in the dictionary, add it\n",
    "        if method not in data_dict:\n",
    "            data_dict[method] = {}\n",
    "\n",
    "        # Add the AMCA to the data dictionary\n",
    "        if knn not in data_dict[method]:\n",
    "            data_dict[method][knn] = []\n",
    "\n",
    "        data_dict[method][knn].append(overall_amca)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for method, knn_values in data_dict.items():\n",
    "        # Prepare data\n",
    "        method = method.replace(\"Random\", \"Default\")\n",
    "        knns = sorted(list(knn_values.keys()))\n",
    "        means = [np.mean(knn_values[knn]) for knn in knns]\n",
    "        mins = [np.min(knn_values[knn]) for knn in knns]\n",
    "        maxs = [np.max(knn_values[knn]) for knn in knns]\n",
    "\n",
    "        overall_avg_amca = np.mean(means)  # Average AMCA over all n_neighbors\n",
    "\n",
    "        print(method)\n",
    "        if 'TADILER' in method:\n",
    "            plt.plot(knns, means, marker='*', linestyle='-', color='#8B0000', \n",
    "                     label=f'{method} (avg: {overall_avg_amca:.3f})', linewidth=2)\n",
    "            plt.fill_between(knns, mins, maxs, color='#8B0000', alpha=0.2)\n",
    "        else:\n",
    "            plt.plot(knns, means, marker='o', linestyle='-', \n",
    "                     label=f'{method} (avg: {overall_avg_amca:.3f})')\n",
    "            plt.fill_between(knns, mins, maxs, alpha=0.2)\n",
    "\n",
    "    plt.xlabel('n_samples')\n",
    "    plt.ylabel('Average AMCA')\n",
    "    plt.title(f'Dataset: Diabetes Retinopathy | Strategy: {strategy_name} ')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(knns)\n",
    "    plt.legend()\n",
    "    plt.savefig(f'tadiler_diabetes_{strategy_name}.pdf', bbox_inches='tight', pad_inches=0)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a17f7e-51ff-4f1e-82da-b9f5b111f3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
